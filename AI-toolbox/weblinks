# AI Web Interaction Snippets - Personal Swiss Army Knife
# Ethical web exploration tools for data collection and site testing

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import time
import re

class PsiWebSnippets:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'PsiOmega WebSnippets v1.0 (Educational/Testing)'
        })
        self.log_file = 'psi-web-exploration.json'
    
    def read_page(self, url, extract_links=True, extract_forms=False):
        """Basic page reading - gets title, content, links"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Extract basic info
            data = {
                'timestamp': datetime.now().isoformat(),
                'url': url,
                'title': soup.title.string if soup.title else 'No Title',
                'status_code': response.status_code,
                'content_length': len(response.text)
            }
            
            # Extract links if requested
            if extract_links:
                links = []
                for a in soup.find_all('a', href=True):
                    links.append({
                        'text': a.get_text(strip=True)[:100],
                        'href': a['href']
                    })
                data['links'] = links[:20]  # Limit to first 20 links
            
            # Extract forms if requested (useful for testing purchase flows)
            if extract_forms:
                forms = []
                for form in soup.find_all('form'):
                    form_data = {
                        'action': form.get('action', ''),
                        'method': form.get('method', 'get'),
                        'inputs': []
                    }
                    for input_tag in form.find_all(['input', 'select', 'textarea']):
                        form_data['inputs'].append({
                            'name': input_tag.get('name', ''),
                            'type': input_tag.get('type', ''),
                            'id': input_tag.get('id', '')
                        })
                    forms.append(form_data)
                data['forms'] = forms
            
            self._log_result(data)
            return data
            
        except Exception as e:
            error_data = {
                'timestamp': datetime.now().isoformat(),
                'url': url,
                'error': str(e),
                'status': 'failed'
            }
            self._log_result(error_data)
            return error_data
    
    def test_link_health(self, url):
        """Check if a link is working - useful for testing purchase flows"""
        try:
            response = self.session.head(url, timeout=5)
            result = {
                'url': url,
                'status_code': response.status_code,
                'working': response.status_code < 400,
                'redirect_chain': [r.url for r in response.history] if response.history else [],
                'final_url': response.url,
                'timestamp': datetime.now().isoformat()
            }
            return result
        except Exception as e:
            return {
                'url': url,
                'error': str(e),
                'working': False,
                'timestamp': datetime.now().isoformat()
            }
    
    def extract_pricing_info(self, url):
        """Extract pricing information from pages"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Look for common price patterns
            price_patterns = [
                r'\$\d+(?:\.\d{2})?',
                r'\d+(?:\.\d{2})?\s*(?:dollars?|USD|\$)',
                r'Price:\s*\$?\d+(?:\.\d{2})?'
            ]
            
            prices = []
            text = soup.get_text()
            
            for pattern in price_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                prices.extend(matches)
            
            return {
                'url': url,
                'prices_found': list(set(prices)),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            return {'url': url, 'error': str(e)}
    
    def monitor_site_changes(self, url, check_interval=300):
        """Monitor a site for changes - useful for watching competitors"""
        try:
            initial_response = self.session.get(url, timeout=10)
            initial_hash = hash(initial_response.text)
            
            print(f"Monitoring {url} for changes...")
            print(f"Initial content hash: {initial_hash}")
            
            while True:
                time.sleep(check_interval)
                try:
                    current_response = self.session.get(url, timeout=10)
                    current_hash = hash(current_response.text)
                    
                    if current_hash != initial_hash:
                        change_data = {
                            'url': url,
                            'change_detected': True,
                            'timestamp': datetime.now().isoformat(),
                            'old_hash': initial_hash,
                            'new_hash': current_hash
                        }
                        self._log_result(change_data)
                        print(f"CHANGE DETECTED at {url}")
                        initial_hash = current_hash
                    else:
                        print(f"No changes detected at {datetime.now().strftime('%H:%M:%S')}")
                        
                except Exception as e:
                    print(f"Error checking {url}: {e}")
                    
        except Exception as e:
            return {'url': url, 'error': str(e)}
    
    def batch_test_urls(self, urls):
        """Test multiple URLs at once - good for checking all your links"""
        results = []
        for url in urls:
            print(f"Testing: {url}")
            result = self.test_link_health(url)
            results.append(result)
            time.sleep(1)  # Be respectful, don't hammer servers
        
        return results
    
    def _log_result(self, data):
        """Log results to file for persistent memory"""
        try:
            with open(self.log_file, 'a') as f:
                json.dump(data, f)
                f.write('\n')
        except Exception as e:
            print(f"Logging error: {e}")

# Example usage functions
def test_fortress_ecosystem():
    """Test your fortress ecosystem URLs"""
    snippets = PsiWebSnippets()
    
    urls_to_test = [
        'https://coldnsteel.github.io/GMSRFC/fortress-grand-opening.html',
        'https://coldnsteel.github.io/GMSRFC/psi-recruits.html',
        'https://coldnsteel.github.io/GMSRFC/Dogpatch-Market/sectors/cashier/index.html',
        'https://coldnsteel.github.io/HackerWatch-Fortress/'
    ]
    
    print("Testing Fortress Ecosystem URLs...")
    results = snippets.batch_test_urls(urls_to_test)
    
    for result in results:
        status = "âœ… Working" if result.get('working', False) else "âŒ Issue"
        print(f"{status}: {result['url']}")
    
    return results

def explore_competitor_site(url):
    """Explore a competitor site ethically"""
    snippets = PsiWebSnippets()
    
    print(f"Exploring: {url}")
    page_data = snippets.read_page(url, extract_links=True, extract_forms=False)
    pricing_data = snippets.extract_pricing_info(url)
    
    print(f"Title: {page_data.get('title', 'Unknown')}")
    print(f"Links found: {len(page_data.get('links', []))}")
    print(f"Prices found: {pricing_data.get('prices_found', [])}")
    
    return page_data, pricing_data

# Quick setup and usage
if __name__ == "__main__":
    print("ðŸ” PsiOmega Web Snippets - Personal AI Swiss Army Knife")
    print("Choose an option:")
    print("1. Test Fortress ecosystem URLs")
    print("2. Explore a competitor site")
    print("3. Monitor site for changes")
    
    choice = input("Enter choice (1-3): ").strip()
    
    if choice == "1":
        test_fortress_ecosystem()
    elif choice == "2":
        url = input("Enter URL to explore: ").strip()
        if url:
            explore_competitor_site(url)
    elif choice == "3":
        url = input("Enter URL to monitor: ").strip()
        if url:
            snippets = PsiWebSnippets()
            snippets.monitor_site_changes(url)
    else:
        print("Invalid choice")
